# -*- coding: utf-8 -*-
"""Project2 Deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h9w-_mr_G8j2j1Xrb2UcJGb_naYbHHjt
"""



import re
import matplotlib.pyplot as plt
import string
from nltk.corpus import stopwords
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize.treebank import TreebankWordDetokenizer
from collections import Counter
from wordcloud import WordCloud
from nltk.corpus import stopwords
import nltk
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import gensim
from sklearn.model_selection import train_test_split
import spacy
import pickle
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt 
import tensorflow as tf
import keras
import numpy as np
import pandas as pd

url = 'https://github.com/HZ004/Project-NLP/blob/main/Product_details.csv?raw=true'
train = pd.read_csv(url)
train

train.head(15)

#Let's get the dataset lenght
len(train)

train['Sentiment'].unique()

train.groupby('Sentiment').nunique()

train["Product_Description"].isnull().sum()

"""
The next steps about data cleaning will be:

    Remove URLs
    Tokenize text
    Remove emails
    Remove new lines characters
    Remove distracting single quotes
    Remove all punctuation signs
    Lowercase all text
    Detokenize text
    Convert list of texts to Numpy array

"""

def depure_data(data):
    
    #Removing URLs with a regular expression
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    data = url_pattern.sub(r'', data)

    # Remove Emails
    data = re.sub('\S*@\S*\s?', '', data)

    # Remove new line characters
    data = re.sub('\s+', ' ', data)

    data = re.sub('@\S*\s?', '', data)
    data = re.sub('\S*#\S*\s?', '', data)
    data = re.sub('#\S*\s?', '', data)
    data = re.sub('&\S*\s?', '', data)

    # Remove distracting single quotes
    data = re.sub("\'", "", data)
        
    return data

temp = []
#Splitting pd.Series to list
data_to_list = train['Product_Description'].values.tolist()
for i in range(len(data_to_list)):
    temp.append(depure_data(data_to_list[i]))
list(temp[:5])

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations
        

data_words = list(sent_to_words(temp))

print(data_words[:10])

len(data_words)

def detokenize(text):
    return TreebankWordDetokenizer().detokenize(text)

data = []
for i in range(len(data_words)):
    data.append(detokenize(data_words[i]))
print(data[:5])

data = np.array(data)

y = np.array(train['Sentiment'])
labels = tf.keras.utils.to_categorical(y, 4, dtype="float32")
del y

x = np.array(train['Product_Type'])
category = tf.keras.utils.to_categorical(x, 10, dtype="float32")
del x

category

len(labels)

"""
Data sequencing and splitting

We'll implement the Keras tokenizer as well as its pad_sequences method to transform our text data into 3D float data, otherwise our neural networks won't be able to be trained on it.
"""

from keras.models import Sequential
from keras import layers
from keras.optimizers import RMSprop,Adam
from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences
from keras import regularizers
from keras import backend as K
from keras.callbacks import ModelCheckpoint
max_words = 5000
max_len = 200

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
reviews = pad_sequences(sequences, maxlen=max_len)
print(reviews)

print(labels)

"""Model building

Applying Deep Learning models using Product Type also.
"""

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data)
sequences = tokenizer.texts_to_sequences(data)
reviews = pad_sequences(sequences, maxlen=max_len)
print(reviews)

# add category to reviews here
reviews = np.append(reviews,category, axis=1)
print(reviews)

#Splitting the data
X_train, X_test, y_train, y_test = train_test_split(reviews,labels, random_state=42)
print (len(X_train),len(X_test),len(y_train),len(y_test))

"""Single LSTM layer model"""

model1 = Sequential()
model1.add(layers.Embedding(max_words, 20))
model1.add(layers.LSTM(15,dropout=0.5))
model1.add(layers.Dense(4,activation='softmax'))

model1.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])

#Implementing model checkpoints to save the best metric and do not lose it on training.
checkpoint1 = ModelCheckpoint("best_model1.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)
history = model1.fit(X_train, y_train, epochs=50,validation_data=(X_test, y_test),callbacks=[checkpoint1])

"""Best model validation"""

#Let's load the best model obtained during training
best_model = keras.models.load_model("best_model1.hdf5")

test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)
print('Model accuracy: ',test_acc)

predictions = best_model.predict(X_test)

"""Confusion matrix

Preparing model
"""



